#!/usr/bin/env python
# coding: utf-8

import re
import os
import csv
import sys
import glob
import tempfile
import fileinput
# import traceback
import subprocess
# from models import SentenceParse
from nltk.tree import ParentedTree
# from sqlalchemy.orm.exc import NoResultFound
from utils import count_lmk_phrases, printcolors, logger

from sqlalchemy import Column,Integer,String
from model import Base, StandardMixin

class SentenceParse(Base, StandardMixin):
    id = Column(Integer, primary_key=True)

    sentence = Column(String)
    original_parse = Column(String)
    modified_parse = Column(String)

    # @classmethod
    # def get_sentence_parse(cls, session, sentence):
    #     q = 

    #     # if orig_parse != None:
    #     #     q = q.filter(SentenceParses.original_parse==orig_parse)

    #     return q

    # @classmethod
    # def add_sentence_parse(cls, sentence, orig_parse, mod_parse):
    #     sp_db = cls.get_sentence_parse(sentence, orig_parse, mod_parse)
    #     if sp_db.count() <= 0:
    #         SentenceParse(sentence=sentence,
    #                       original_parse=orig_parse,
    #                       modified_parse=mod_parse)
    #         # session().commit()

    # @classmethod
    # def add_sentence_parse_blind(cls, sentence, orig_parse, mod_parse):
    #     SentenceParse(sentence=sentence,
    #                   original_parse=orig_parse,
    #                   modified_parse=mod_parse)

def chunks(l, n):
    return [l[i:i+n] for i in range(0, len(l), n)]

def parse_sentences(ss, parser_path='../bllip-parser', n=5, threads=2):
    """parse sentences with the charniak parser"""
    # create a temporary file and write the sentences in it
    temp = tempfile.NamedTemporaryFile()
    for s in ss:
        temp.write('<s> %s </s>\n' % s)
    temp.flush()
    # where am i?
    # prev_path = os.getcwd()
    # get into the charniak parser directory
    # os.chdir(parser_path)
    # call the parser
    proc = subprocess.Popen(['./parse.sh', '-N%i' % n, temp.name],
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE)
    # capture output
    output = proc.communicate()[0]
    # return to where i was
    # os.chdir(prev_path)
    # get rid of temporary file
    temp.close()
    # return the parse trees
    return chunks([l for l in output.splitlines() if l[:3] == '(S1'],n)




def modify_parses(trees, tregex_path='stanford-tregex',
                         surgery_path='surgery'):
    """modify parse trees using tsurgeon"""
    # write trees to temp file
    n = len(trees[0])
    temp = tempfile.NamedTemporaryFile()
    for t in trees:
        for tt in t:
            temp.write(tt + '\n')
    temp.flush()

    # tregex jar location
    jar = os.path.join(tregex_path, 'stanford-tregex.jar')
    tsurgeon = 'edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon'
    # surgery scripts
    surgery = sorted(glob.glob(os.path.join(surgery_path, '*')))
    proc = subprocess.Popen(['java', '-mx100m', '-cp', jar, tsurgeon,
                             '-s', '-treeFile', temp.name] + surgery,
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE)
    # get output
    output = proc.communicate()[0]
    # delete temp file
    temp.close()
    # return modified parse trees
    return chunks(output.splitlines(),n)



def parse_generator_data(datafile):
    """parse a file with data generated by table2d generator"""
    xlocs, ylocs, sentences = [], [], []
    # this is how each observation looks like
    # for example: Vec2(5.31, 5.11); on the table
    pattern = r'^Vec2\((?P<x>-?[0-9.]+), (?P<y>-?[0-9.]+)\); (?P<sent>.+)$'
    for line in datafile:
        match = re.match(pattern, line)
        if match:
            xlocs.append(float(match.group('x')))
            ylocs.append(float(match.group('y')))
            sentences.append(match.group('sent'))
    return xlocs, ylocs, sentences


class ParseError(RuntimeError):
    pass

def get_modparse(session, sentence):
    """returns the modified parse tree for a sentence"""
    sp_db = session.query(SentenceParse).filter(SentenceParse.sentence==sentence)
    try:
        res = sp_db.all()[0]
        parsetree = res.original_parse
        modparsetree = res.modified_parse
    except:
        parses = parse_sentences([sentence])
        if len(parses) == 0:
            raise ParseError(printcolors.WARNING + ('ParseError: a sentence was empty'))
        modparses = modify_parses(parses)

        for i,chunk in enumerate(modparses[:]):
            for j,modparse in enumerate(chunk):
                if 'LANDMARK-PHRASE' in modparse:
                    modparses[i] = modparse
                    parses[i] = parses[i][j]
                    break
            if isinstance(modparses[i],list):
                modparses[i] = modparses[i][0]
                parses[i] = parses[i][0]

        parsetree = parses[0]
        modparsetree = modparses[0]
        modparsetree = number_landmark_phrases(modparsetree)
        try:
            sp = SentenceParse(sentence=sentence,
                               original_parse=parsetree,
                               modified_parse=modparsetree)
            session.add(sp)
            session.commit()
        except Exception as e:
            print e

    if count_lmk_phrases(ParentedTree.parse(modparsetree)) < 1:
        raise ParseError(printcolors.WARNING + ('ParseError: Parse contained no Landmark phrase.\nSentence: %s\nParse: %s\nModparse: %s' % (sentence,parsetree,modparsetree)))

    return parsetree, modparsetree

def number_landmark_phrases(modparse):
    oldmodparse = ''
    i = 0
    while oldmodparse != modparse:
        oldmodparse = modparse
        modparse = re.sub(r'\(LANDMARK-PHRASE ', '(LANDMARK-PHRASE-%s ' % i,modparse,count=1)
        modparse = re.sub(r'\(LANDMARK ', '(LANDMARK-%s ' % i,modparse,count=1)
        i += 1
    return modparse

if __name__ == '__main__':
    # parse data from file or stdin
    xlocs, ylocs, sentences = parse_generator_data(fileinput.input())
    # parse sentences
    parses = parse_sentences(sentences)
    # write csv data to stdout
    writer = csv.writer(sys.stdout, lineterminator='\n')
    writer.writerow(['xloc', 'yloc', 'sentence', 'parse'])
    for row in zip(xlocs, ylocs, sentences, parses):
        writer.writerow(row)
